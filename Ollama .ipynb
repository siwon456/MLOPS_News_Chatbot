{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185540f-012e-429d-843b-ccad830f28dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "# LangChain v0.2.x에서는 'from langchain.chains import ConversationalRetrievalChain' 대신\n",
    "# 'langchain.chains.retrieval_qa'를 사용하고 memory를 직접 주입하는 방식이 더 흔합니다.\n",
    "\n",
    "def run_rag_chatbot(keyword):\n",
    "    \"\"\"\n",
    "    RAG 챗봇을 실행하고 사용자의 질문에 답변합니다. (메모리 기능 추가)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. 벡터 데이터베이스 불러오기\n",
    "        db_path = f\"./chroma_db_{keyword}\"\n",
    "        client = chromadb.PersistentClient(path=db_path)\n",
    "        \n",
    "        collection_name = f\"{keyword}_news_collection\"\n",
    "        \n",
    "        # 2. 임베딩 모델 로드\n",
    "        embeddings_model = SentenceTransformerEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n",
    "        \n",
    "        # 3. LangChain의 Chroma 클래스로 벡터스토어 로드\n",
    "        vectorstore = Chroma(\n",
    "            client=client,\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=embeddings_model\n",
    "        )\n",
    "\n",
    "        # 4. LangChain 리트리버 설정\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\", \n",
    "            search_kwargs={\"k\": 3} \n",
    "        )\n",
    "        \n",
    "        # 5. Ollama를 통해 로컬 LLM 로드\n",
    "        llm = Ollama(model=\"llama3\")\n",
    "\n",
    "        # 6. 대화 메모리 추가\n",
    "        # 이전 대화 내용을 저장하여 LLM에게 전달하는 역할\n",
    "        memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\", \n",
    "            return_messages=True\n",
    "        )\n",
    "\n",
    "        # 7. RetrievalQA 체인 구성 (메모리 추가)\n",
    "        # 이 부분이 수정되었습니다.\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            memory=memory # 메모리 객체를 체인에 주입\n",
    "        )\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(\"챗봇이 준비되었습니다. '종료'를 입력하면 대화가 종료됩니다.\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        while True:\n",
    "            question = input(\"질문: \").strip()\n",
    "            if question.lower() == \"종료\":\n",
    "                print(\"챗봇을 종료합니다.\")\n",
    "                break\n",
    "            \n",
    "            # 8. 질문에 대한 답변 생성 및 출력\n",
    "            # 이제 qa_chain이 대화 기록을 자동으로 관리합니다.\n",
    "            response = qa_chain.invoke({\"query\": question})\n",
    "            print(\"챗봇: \" + response['result'])\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류가 발생했습니다: {e}\")\n",
    "        print(\"Ollama가 정상적으로 실행 중인지 확인하세요. (ollama run llama3)\")\n",
    "\n",
    "# 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    search_keyword = input(\"챗봇에 사용할 데이터의 키워드(영문)를 입력하세요: \").strip()\n",
    "    run_rag_chatbot(search_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba5d0c-aa6e-4c1b-a0ea-f613549b2ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
